{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3150fd-5e22-4eee-856d-398aba000ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download essential NLTK data packages\n",
    "print(\"Downloading NLTK data packages...\")\n",
    "\n",
    "# Download the most commonly used packages\n",
    "nltk.download('punkt')           # Tokenizer\n",
    "nltk.download('stopwords')       # Stopwords list\n",
    "nltk.download('wordnet')         # WordNet lexical database\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
    "nltk.download('maxent_ne_chunker')  # Named entity chunker\n",
    "nltk.download('words')           # Word list\n",
    "\n",
    "print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b953ae91-3b12-4c6a-b539-6595ccd22897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular NLTK packages downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download all popular NLTK packages (this will take longer)\n",
    "nltk.download('popular')\n",
    "print(\"Popular NLTK packages downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf044a1-5d48-40a0-a684-da2c27c57565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Permanent NLTK data path configuration\n",
    "nltk.data.path = [r'C:\\Users\\aryan\\AppData\\Roaming\\nltk_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "458e0f39-f482-41b2-ad72-50bfb40463b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cmudict', 'cmudict.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'inaugural', 'inaugural.zip', 'movie_reviews', 'movie_reviews.zip', 'names', 'names.zip', 'omw-1.4.zip', 'omw.zip', 'shakespeare', 'shakespeare.zip', 'stopwords', 'stopwords.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ceaed7-27cf-4f65-8139-c529a4599f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ddc9d10-0cb5-4582-934d-920a8b26c1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "511cf4fd-cbf6-49fd-bc40-e822248bc77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf33579-fa28-4fc3-a98a-053885cd2cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet [:500]:\n",
    "    print(word,sep=' ', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e4d1285-2432-44aa-ab29-95be62e3c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = \"\"\"Artificial intelligence (AI) is a set of technologies that empowers computers to learn, reason, and perform a variety of advanced tasks in ways that used to require human intelligence, such as understanding language, analyzing data, and even providing helpful suggestions. It’s a transformational technology that can bring meaningful and positive change to people and societies and the world.\n",
    "\n",
    "It encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology. \n",
    "\n",
    "AI is about teaching computers to do the amazing things our own brains can do, from understanding the world around them to learning new things and even coming up with fresh ideas. For instance, AI is used in optical character recognition (OCR) to pull text and data from various images and documents. This process transforms unstructured content into structured, business-ready data, helping uncover valuable insights.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aca7d82-a774-40cb-9dc5-b920c333c064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d11784-271c-4a76-8317-f2f32a4b5e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9601856-4cc3-4ce9-8d73-8f22cab456f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'that',\n",
       " 'empowers',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'learn',\n",
       " ',',\n",
       " 'reason',\n",
       " ',',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'advanced',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'ways',\n",
       " 'that',\n",
       " 'used',\n",
       " 'to',\n",
       " 'require',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'understanding',\n",
       " 'language',\n",
       " ',',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " ',',\n",
       " 'and',\n",
       " 'even',\n",
       " 'providing',\n",
       " 'helpful',\n",
       " 'suggestions',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'transformational',\n",
       " 'technology',\n",
       " 'that',\n",
       " 'can',\n",
       " 'bring',\n",
       " 'meaningful',\n",
       " 'and',\n",
       " 'positive',\n",
       " 'change',\n",
       " 'to',\n",
       " 'people',\n",
       " 'and',\n",
       " 'societies',\n",
       " 'and',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'It',\n",
       " 'encompasses',\n",
       " 'many',\n",
       " 'different',\n",
       " 'disciplines',\n",
       " ',',\n",
       " 'including',\n",
       " 'computer',\n",
       " 'science',\n",
       " ',',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'and',\n",
       " 'statistics',\n",
       " ',',\n",
       " 'hardware',\n",
       " 'and',\n",
       " 'software',\n",
       " 'engineering',\n",
       " ',',\n",
       " 'linguistics',\n",
       " ',',\n",
       " 'neuroscience',\n",
       " ',',\n",
       " 'and',\n",
       " 'even',\n",
       " 'philosophy',\n",
       " 'and',\n",
       " 'psychology',\n",
       " '.',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'about',\n",
       " 'teaching',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'amazing',\n",
       " 'things',\n",
       " 'our',\n",
       " 'own',\n",
       " 'brains',\n",
       " 'can',\n",
       " 'do',\n",
       " ',',\n",
       " 'from',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'world',\n",
       " 'around',\n",
       " 'them',\n",
       " 'to',\n",
       " 'learning',\n",
       " 'new',\n",
       " 'things',\n",
       " 'and',\n",
       " 'even',\n",
       " 'coming',\n",
       " 'up',\n",
       " 'with',\n",
       " 'fresh',\n",
       " 'ideas',\n",
       " '.',\n",
       " 'For',\n",
       " 'instance',\n",
       " ',',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'used',\n",
       " 'in',\n",
       " 'optical',\n",
       " 'character',\n",
       " 'recognition',\n",
       " '(',\n",
       " 'OCR',\n",
       " ')',\n",
       " 'to',\n",
       " 'pull',\n",
       " 'text',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'various',\n",
       " 'images',\n",
       " 'and',\n",
       " 'documents',\n",
       " '.',\n",
       " 'This',\n",
       " 'process',\n",
       " 'transforms',\n",
       " 'unstructured',\n",
       " 'content',\n",
       " 'into',\n",
       " 'structured',\n",
       " ',',\n",
       " 'business-ready',\n",
       " 'data',\n",
       " ',',\n",
       " 'helping',\n",
       " 'uncover',\n",
       " 'valuable',\n",
       " 'insights',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokens = word_tokenize(AI)\n",
    "AI_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb574e6b-c04a-458e-b851-aaf15731b49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2706738-c16a-464b-b889-a0bf980312c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dff0b1b-7fc6-4f12-a32c-cb60cd99bb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 15, 'and': 12, 'to': 6, '.': 6, 'data': 4, 'ai': 3, 'is': 3, 'a': 3, 'that': 3, 'even': 3, ...})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c5877ca-5dff-44c5-87e1-1a15076d758c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ac9909b-f97e-4d2a-8c5e-e16ef25bcbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34fe6f7f-73d0-42db-94e7-31aff45c6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 15),\n",
       " ('and', 12),\n",
       " ('to', 6),\n",
       " ('.', 6),\n",
       " ('data', 4),\n",
       " ('ai', 3),\n",
       " ('is', 3),\n",
       " ('a', 3),\n",
       " ('that', 3),\n",
       " ('even', 3)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d01be563-87bd-45a3-8d55-ee8b91d8e86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank = blankline_tokenize(AI)\n",
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37de5e93-309f-4f6e-92b3-6ae7ef50e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI is about teaching computers to do the amazing things our own brains can do, from understanding the world around them to learning new things and even coming up with fresh ideas. For instance, AI is used in optical character recognition (OCR) to pull text and data from various images and documents. This process transforms unstructured content into structured, business-ready data, helping uncover valuable insights.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_blank[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b79b536-9b54-4da6-9583-ace67df5bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cf83ddd-1301-4c19-8ca3-cf5bdffbd9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'be',\n",
       " 'beautiful',\n",
       " 'means',\n",
       " 'to',\n",
       " 'be',\n",
       " 'yourself',\n",
       " '.',\n",
       " 'You',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'accepted',\n",
       " 'by',\n",
       " 'others',\n",
       " '.',\n",
       " 'You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'accept',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"To be beautiful means to be yourself. You don’t need to be accepted by others. You need to accept yourself\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "398d1fd1-9418-41de-ac08-3d213f670ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'be'),\n",
       " ('be', 'beautiful'),\n",
       " ('beautiful', 'means'),\n",
       " ('means', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'yourself'),\n",
       " ('yourself', '.'),\n",
       " ('.', 'You'),\n",
       " ('You', 'don'),\n",
       " ('don', '’'),\n",
       " ('’', 't'),\n",
       " ('t', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'accepted'),\n",
       " ('accepted', 'by'),\n",
       " ('by', 'others'),\n",
       " ('others', '.'),\n",
       " ('.', 'You'),\n",
       " ('You', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'accept'),\n",
       " ('accept', 'yourself')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e779c227-659c-448c-931c-fd5091b2fbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'be', 'beautiful'),\n",
       " ('be', 'beautiful', 'means'),\n",
       " ('beautiful', 'means', 'to'),\n",
       " ('means', 'to', 'be'),\n",
       " ('to', 'be', 'yourself'),\n",
       " ('be', 'yourself', '.'),\n",
       " ('yourself', '.', 'You'),\n",
       " ('.', 'You', 'don'),\n",
       " ('You', 'don', '’'),\n",
       " ('don', '’', 't'),\n",
       " ('’', 't', 'need'),\n",
       " ('t', 'need', 'to'),\n",
       " ('need', 'to', 'be'),\n",
       " ('to', 'be', 'accepted'),\n",
       " ('be', 'accepted', 'by'),\n",
       " ('accepted', 'by', 'others'),\n",
       " ('by', 'others', '.'),\n",
       " ('others', '.', 'You'),\n",
       " ('.', 'You', 'need'),\n",
       " ('You', 'need', 'to'),\n",
       " ('need', 'to', 'accept'),\n",
       " ('to', 'accept', 'yourself')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc8412b0-392a-4a63-913a-9099011db37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'be', 'beautiful', 'means', 'to'),\n",
       " ('be', 'beautiful', 'means', 'to', 'be'),\n",
       " ('beautiful', 'means', 'to', 'be', 'yourself'),\n",
       " ('means', 'to', 'be', 'yourself', '.'),\n",
       " ('to', 'be', 'yourself', '.', 'You'),\n",
       " ('be', 'yourself', '.', 'You', 'don'),\n",
       " ('yourself', '.', 'You', 'don', '’'),\n",
       " ('.', 'You', 'don', '’', 't'),\n",
       " ('You', 'don', '’', 't', 'need'),\n",
       " ('don', '’', 't', 'need', 'to'),\n",
       " ('’', 't', 'need', 'to', 'be'),\n",
       " ('t', 'need', 'to', 'be', 'accepted'),\n",
       " ('need', 'to', 'be', 'accepted', 'by'),\n",
       " ('to', 'be', 'accepted', 'by', 'others'),\n",
       " ('be', 'accepted', 'by', 'others', '.'),\n",
       " ('accepted', 'by', 'others', '.', 'You'),\n",
       " ('by', 'others', '.', 'You', 'need'),\n",
       " ('others', '.', 'You', 'need', 'to'),\n",
       " ('.', 'You', 'need', 'to', 'accept'),\n",
       " ('You', 'need', 'to', 'accept', 'yourself')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "544ef548-cfab-415e-a942-6243c74b9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f06a065-ef62-4083-aa6b-6cf21ae0cb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59588eb7-55f7-4f6d-bdd1-c2e810fc2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = [\"give\", \"giving\", \"given\", \"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words + \":\" + pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b46e23c-e165-4fd7-98cc-9b679b256840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer \n",
    "lst = LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words + \":\" + lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c339641b-82dd-486d-9c0f-dba17275edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d9b26e8-6ffb-4131-8503-d0c7b0f201f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words + \":\" + sbst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83585ecb-724a-40de-a389-f67bffed7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f93cba48-98ee-45ab-86fd-a3692928c20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c4694fd-ebe5-412b-89c0-644d0d67a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words + ':' + word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79ea45ae-3da3-4b22-aa43-2e894cdf05f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Now this will work\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0bd3e1d-49b1-4111-af46-05d0ed210803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bafe248c-a858-457c-9b44-e383c5b562e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 15),\n",
       " ('and', 12),\n",
       " ('to', 6),\n",
       " ('.', 6),\n",
       " ('data', 4),\n",
       " ('ai', 3),\n",
       " ('is', 3),\n",
       " ('a', 3),\n",
       " ('that', 3),\n",
       " ('even', 3)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e4acef5-2edc-4b37-b1e0-7790a1d3d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9024bd3-6260-4a1b-9696-1a0de34ce27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation = []\n",
    "for words "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
